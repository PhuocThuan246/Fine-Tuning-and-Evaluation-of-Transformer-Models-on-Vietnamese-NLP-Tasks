# Fine-Tuning-and-Evaluation-of-Transformer-Models-on-Vietnamese-NLP-Tasks
# ğŸ‡»ğŸ‡³ Fine-tune & ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh Transformer trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t  
# Giá»›i thiá»‡u dá»± Ã¡n  
Repository nÃ y chá»©a mÃ£ nguá»“n, bÃ¡o cÃ¡o vÃ  tÃ i liá»‡u minh há»a cho **ba mÃ´ hÃ¬nh Transformer** Ä‘Æ°á»£c fine-tune trÃªn **dá»¯ liá»‡u tiáº¿ng Viá»‡t**, gá»“m:

1. **PhoBERT** â€“ PhÃ¢n loáº¡i cáº£m xÃºc vÄƒn báº£n  
2. **ViT5** â€“ Dá»‹ch mÃ¡y Anh â†’ Viá»‡t  
3. **Qwen3-0.6B** â€“ Há»i Ä‘Ã¡p tiáº¿ng Viá»‡t  

Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng theo hÆ°á»›ng **so sÃ¡nh mÃ´ hÃ¬nh gá»‘c vs. mÃ´ hÃ¬nh má»Ÿ rá»™ng báº±ng Synthetic Data**, nháº±m Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u tá»•ng há»£p Ä‘á»‘i vá»›i hiá»‡u quáº£ mÃ´ hÃ¬nh.

---

# 1. PhoBERT â€“ PhÃ¢n loáº¡i cáº£m xÃºc
### Giá»›i thiá»‡u mÃ´ hÃ¬nh  
PhoBERT lÃ  mÃ´ hÃ¬nh Transformer encoder-only dÃ nh cho tiáº¿ng Viá»‡t, tÆ°Æ¡ng tá»± RoBERTa, Ä‘Æ°á»£c pre-train trÃªn >20GB dá»¯ liá»‡u tiáº¿ng Viá»‡t.

### Dá»¯ liá»‡u  
- Bá»™ dá»¯ liá»‡u: **VLSP2016**  
- Train: 4590 máº«u  
- Validation: 510 máº«u  
- Test: 1050 máº«u  
- Ba lá»›p cÃ¢n báº±ng: Positive â€“ Neutral â€“ Negative  

### Synthetic Data  
Sinh bá»Ÿi **Qwen2-7B-Instruct** vá»›i ba ká»¹ thuáº­t:
- Paraphrase Sentiment  
- Similar Sentiment Generation  
- Dataset Expansion  

### Quy trÃ¬nh fine-tune  
- Learning rate: 2e-5  
- Batch size: 16  
- Epochs: 4  
- Weight decay: 0.01  
- Trainer: HuggingFace Trainer  

### Káº¿t quáº£  
ÄÃ¡nh giÃ¡ theo: Accuracy, F1-score  
KÃ¨m Ä‘Ã¡nh giÃ¡ bá»Ÿi LLM Ä‘á»ƒ kiá»ƒm chá»©ng cháº¥t lÆ°á»£ng ngá»¯ nghÄ©a.

---

# 2. ViT5 â€“ Dá»‹ch mÃ¡y Anh â†’ Viá»‡t  
### Giá»›i thiá»‡u mÃ´ hÃ¬nh  
ViT5 (VietAI/vit5-base) lÃ  mÃ´ hÃ¬nh seq2seq thuáº§n Viá»‡t (Encoderâ€“Decoder), dá»±a trÃªn Google T5.

### Dá»¯ liá»‡u  
- harouzie/vi_en-translation  
- 5000 cáº·p song ngá»¯ ENâ€“VI  
- TÃ¡ch táº­p: Train/Valid/Test  

### Synthetic Data  
Sinh bá»Ÿi Qwen thÃ´ng qua:
- Paraphrase + Translate  
- Similar sentence generation  
- Dataset expansion  

### Quy trÃ¬nh fine-tune  
- LR: 2e-4  
- Epochs: 3  
- Gradient accumulation: 4  
- Warmup ratio: 0.03  
- Trainer: Seq2SeqTrainer  
- Prefix input: `translate English to Vietnamese:`

### Káº¿t quáº£  
ÄÃ¡nh giÃ¡ theo BLEU + Ä‘Ã¡nh giÃ¡ LLM: meaning + fluency.

---

# 3. Qwen3-0.6B â€“ Há»i Ä‘Ã¡p tiáº¿ng Viá»‡t  
### Giá»›i thiá»‡u mÃ´ hÃ¬nh  
Qwen3-0.6B lÃ  mÃ´ hÃ¬nh decoder-only (giá»‘ng GPT) tá»‘i Æ°u cho fine-tune trÃªn GPU táº§m trung.

### Dá»¯ liá»‡u  
- UIT-ViQuAD 2.0  
- 6000+ máº«u QA thuá»™c 31 chá»§ Ä‘á»  
- Train: 4500  
- Validation: 500  
- Test: 1000  

### Synthetic Data  
Sinh báº±ng Qwen2-7B vá»›i:
- Sinh Ä‘oáº¡n vÄƒn má»›i + cÃ¢u há»i má»›i
- Loáº¡i trÃ¹ng, kiá»ƒm tra thá»§ cÃ´ng  
- Tá»•ng train tÄƒng lÃªn ~6500 máº«u  

### Quy trÃ¬nh fine-tune  
- Epochs: 1  
- LR: 2e-4  
- Max length: 1024  
- Trainer: **SFTTrainer (HuggingFace TRL)**  
- Format dá»¯ liá»‡u: Chat format (user â†’ assistant)

### Káº¿t quáº£  
ÄÃ¡nh giÃ¡:
- Exact Match  
- F1-score  
- ÄÃ¡nh giÃ¡ bá»Ÿi LLM (tÃ­nh máº¡ch láº¡c, tÃ­nh há»£p lÃ½)

---

# CÃ´ng nghá»‡ sá»­ dá»¥ng  
- Python 3.10  
- PyTorch  
- HuggingFace Transformers  
- HuggingFace Datasets  
- TRL (Transformers Reinforcement Learning)  
- Qwen LLM  
- Matplotlib / Seaborn  

